{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd063a8339a2b998405504b80cac15dbfc0480b6c1c806e16474572f1b40b735ee0",
   "display_name": "Python 3.9.5 64-bit ('env': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "63a8339a2b998405504b80cac15dbfc0480b6c1c806e16474572f1b40b735ee0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 2000\n",
    "SEQUENCE_LENGTH = 500\n",
    "\n",
    "def create_model(max_tokens=None):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "    layers.Embedding(max_tokens or MAX_TOKENS, output_dim = 30, name=\"embedding\"),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(8)]\n",
    "    )\n",
    "    model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    model.load_weights('../checkpoints/my_checkpoint_20')\n",
    "\n",
    "    return model\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS, # only consider this many words\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH) \n",
    "\n",
    "def vectorize_movie_scripts(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def stringProcessing(s):\n",
    "    s = re.sub(r\"\\'\", \"\", s)\n",
    "    s = re.sub(r'\\n', ' ', s)\n",
    "    s = re.sub(r'\\t', '', s)\n",
    "    s = re.sub(r\"\\[[^[]*\\]\", '', s)\n",
    "    s = re.sub(r'[^\\w\\s]', ' ', s)\n",
    "    s = re.sub(r' +', ' ', s)\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bourne.txt [ 0.1857727  -0.70903105  0.42797962 -0.03703333 -0.23253712  0.44204602\n",
      " -1.3601608  -4.934482  ]\n",
      "deadpoets.txt [-1.7333281   0.5443739   1.4926052  -1.312813   -1.9460292   0.8860587\n",
      " -0.69746614 -4.9762983 ]\n",
      "fellowship.txt [-1.6093009   0.91127765 -0.7827016   0.51166475  0.3851266  -0.86100215\n",
      " -1.2381443  -5.4872737 ]\n",
      "forrest.txt [ 2.7013892e-01 -7.4422103e-01  3.7579489e-01 -2.4289940e-01\n",
      " -1.4102459e-03  5.6090569e-01 -1.5939732e+00 -4.9245977e+00]\n",
      "goodwillhunting.txt [-0.46705815  2.3599677  -0.41788223 -2.4510686  -2.2347584   0.890132\n",
      " -1.8238227  -5.6453047 ]\n",
      "incredibles.txt [ 0.15989065 -1.9651154   0.7284904   0.97542316  0.11404774  0.60787\n",
      " -1.0682911  -4.7816663 ]\n",
      "jedi.txt [ 0.5676284   1.2058978  -0.2627478  -1.1515511  -1.1492127  -0.53111666\n",
      " -2.7649083  -5.876196  ]\n",
      "khan.txt [ 0.745964   -0.13054618  0.11489633 -1.0392745  -0.4346859   0.51892954\n",
      " -2.2972257  -5.37253   ]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "shawshank.txt [-0.80076295  1.2794873   0.34279975 -1.1230642  -1.2350041   0.11890902\n",
      " -2.3969839  -5.6252737 ]\n",
      "titanic.txt [ 0.448355   0.5325455  0.0066276 -1.6959094 -1.4742285  1.1439179\n",
      " -2.4554396 -5.7488704]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model = create_model() \n",
    "\n",
    "# df = pd.read_csv(\"https://raw.githubusercontent.com/benbrill/MoodSpace/main/data/trainingSongs_clean.csv\")\n",
    "\n",
    "# data = tf.data.Dataset.from_tensor_slices((df[\"lyrics\"]))\n",
    "# data_vec = data.map(vectorize_headline)\n",
    "# df = pd.DataFrame(model.predict(data_vec))\n",
    "# df.shape\n",
    "d = {}\n",
    "for script_path in os.listdir(\"../scripts\"):\n",
    "    with open(f\"../scripts/{script_path}\") as f:\n",
    "        contents = f.read()\n",
    "\n",
    "        contents = stringProcessing(contents)\n",
    "\n",
    "        df = pd.DataFrame({\"lyrics\": [contents]})\n",
    "        \n",
    "\n",
    "\n",
    "        data = tf.data.Dataset.from_tensor_slices((df[\"lyrics\"]))\n",
    "        vectorize_layer.adapt(data)\n",
    "        data_vec = data.map(vectorize_movie_scripts)\n",
    "        d[script_path] = model.predict(data_vec)[0]\n",
    "        print(script_path, model.predict(data_vec)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(264, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bourne.txt': array([ 0.1857727 , -0.70903105,  0.42797962, -0.03703333, -0.23253712,\n",
       "         0.44204602, -1.3601608 , -4.934482  ], dtype=float32),\n",
       " 'deadpoets.txt': array([-1.7333281 ,  0.5443739 ,  1.4926052 , -1.312813  , -1.9460292 ,\n",
       "         0.8860587 , -0.69746614, -4.9762983 ], dtype=float32),\n",
       " 'fellowship.txt': array([-1.6093009 ,  0.91127765, -0.7827016 ,  0.51166475,  0.3851266 ,\n",
       "        -0.86100215, -1.2381443 , -5.4872737 ], dtype=float32),\n",
       " 'forrest.txt': array([ 2.7013892e-01, -7.4422103e-01,  3.7579489e-01, -2.4289940e-01,\n",
       "        -1.4102459e-03,  5.6090569e-01, -1.5939732e+00, -4.9245977e+00],\n",
       "       dtype=float32),\n",
       " 'goodwillhunting.txt': array([-0.46705815,  2.3599677 , -0.41788223, -2.4510686 , -2.2347584 ,\n",
       "         0.890132  , -1.8238227 , -5.6453047 ], dtype=float32),\n",
       " 'incredibles.txt': array([ 0.15989065, -1.9651154 ,  0.7284904 ,  0.97542316,  0.11404774,\n",
       "         0.60787   , -1.0682911 , -4.7816663 ], dtype=float32),\n",
       " 'jedi.txt': array([ 0.5676284 ,  1.2058978 , -0.2627478 , -1.1515511 , -1.1492127 ,\n",
       "        -0.53111666, -2.7649083 , -5.876196  ], dtype=float32),\n",
       " 'khan.txt': array([ 0.745964  , -0.13054618,  0.11489633, -1.0392745 , -0.4346859 ,\n",
       "         0.51892954, -2.2972257 , -5.37253   ], dtype=float32),\n",
       " 'shawshank.txt': array([-0.80076295,  1.2794873 ,  0.34279975, -1.1230642 , -1.2350041 ,\n",
       "         0.11890902, -2.3969839 , -5.6252737 ], dtype=float32),\n",
       " 'titanic.txt': array([ 0.448355 ,  0.5325455,  0.0066276, -1.6959094, -1.4742285,\n",
       "         1.1439179, -2.4554396, -5.7488704], dtype=float32)}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}