{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd06a4c8c1c3e5315735c64f2e463df52f93ec7bac23c7ea2a067534b2f555fbf5f",
   "display_name": "Python 3.7.10 64-bit ('PIC16B': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.7444502   0.01933315 -0.02601114  0.4542644  -0.05159525  0.38804668\n -0.9677498  -5.6487317 ]\n"
     ]
    }
   ],
   "source": [
    "# only the top distinct words will be tracked\n",
    "MAX_TOKENS = 2000\n",
    "\n",
    "# each headline will be a vector of length 25\n",
    "SEQUENCE_LENGTH = 500\n",
    "\n",
    "def create_model(max_tokens=None):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "    layers.Embedding(max_tokens or MAX_TOKENS, output_dim = 3, name=\"embedding\"),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(8)]\n",
    "    )\n",
    "    model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    model.load_weights('../checkpoints/my_checkpoint')\n",
    "\n",
    "    return model\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS, # only consider this many words\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH) \n",
    "\n",
    "def vectorize_headline(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def stringProcessing(s):\n",
    "    s = re.sub(r\"\\'\", \"\", s)\n",
    "    s = re.sub(r'\\n', ' ', s)\n",
    "    s = re.sub(r'\\t', '', s)\n",
    "    s = re.sub(r\"\\[[^[]*\\]\", '', s)\n",
    "    s = re.sub(r'[^\\w\\s]', ' ', s)\n",
    "    s = re.sub(r' +', ' ', s)\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bourne.txt [ 0.74069095  0.02461722 -0.02864002  0.447855   -0.05092469  0.38677678\n",
      " -0.9669975  -5.6361594 ]\n",
      "deadpoets.txt [ 0.1899558   0.7987493  -0.4137746  -0.49113056  0.04731353  0.20073612\n",
      " -0.85679483 -3.7943552 ]\n",
      "fellowship.txt [ 0.1899558   0.7987493  -0.4137746  -0.49113056  0.04731353  0.20073612\n",
      " -0.85679483 -3.7943552 ]\n",
      "forrest.txt [ 0.69182     0.0933111  -0.06281567  0.3645321  -0.04220729  0.37026793\n",
      " -0.95721817 -5.47272   ]\n",
      "goodwillhunting.txt [ 0.1899558   0.7987493  -0.4137746  -0.49113056  0.04731353  0.20073612\n",
      " -0.85679483 -3.7943552 ]\n",
      "incredibles.txt [ 0.8534694  -0.13390633  0.05022651  0.6401378  -0.07104169  0.42487407\n",
      " -0.98956513 -6.0133276 ]\n",
      "jedi.txt [ 0.1899558   0.7987493  -0.4137746  -0.49113056  0.04731353  0.20073612\n",
      " -0.85679483 -3.7943552 ]\n",
      "khan.txt [ 0.5809208   0.24919447 -0.14036848  0.17545265 -0.02242547  0.33280563\n",
      " -0.93502694 -5.101842  ]\n",
      "shawshank.txt [ 0.2632618   0.695708   -0.36251104 -0.3661462   0.03423746  0.22549914\n",
      " -0.87146354 -4.03951   ]\n",
      "titanic.txt [ 0.40423483  0.49755082 -0.26392692 -0.12579146  0.00909117  0.27312043\n",
      " -0.8996721  -4.510959  ]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model = create_model() \n",
    "\n",
    "# df = pd.read_csv(\"https://raw.githubusercontent.com/benbrill/MoodSpace/main/data/trainingSongs_clean.csv\")\n",
    "\n",
    "# data = tf.data.Dataset.from_tensor_slices((df[\"lyrics\"]))\n",
    "# data_vec = data.map(vectorize_headline)\n",
    "# df = pd.DataFrame(model.predict(data_vec))\n",
    "# df.shape\n",
    "for script_path in os.listdir(\"../scripts\"):\n",
    "    with open(f\"../scripts/{script_path}\") as f:\n",
    "        contents = f.read()\n",
    "\n",
    "        contents = stringProcessing(contents)\n",
    "\n",
    "        df = pd.DataFrame({\"lyrics\": [contents]})\n",
    "        \n",
    "\n",
    "\n",
    "        data = tf.data.Dataset.from_tensor_slices((df[\"lyrics\"]))\n",
    "\n",
    "        data_vec = data.map(vectorize_headline)\n",
    "\n",
    "        print(script_path, model.predict(data_vec)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(264, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}