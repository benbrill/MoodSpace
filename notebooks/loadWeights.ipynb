{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd063a8339a2b998405504b80cac15dbfc0480b6c1c806e16474572f1b40b735ee0",
   "display_name": "Python 3.9.5 64-bit ('env': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "63a8339a2b998405504b80cac15dbfc0480b6c1c806e16474572f1b40b735ee0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 2000\n",
    "SEQUENCE_LENGTH = 500\n",
    "\n",
    "def create_model(max_tokens=None):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "    layers.Embedding(max_tokens or MAX_TOKENS, output_dim = 3, name=\"embedding\"),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(8)]\n",
    "    )\n",
    "    model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    model.load_weights('../checkpoints/my_checkpoint')\n",
    "\n",
    "    return model\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS, # only consider this many words\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH) \n",
    "\n",
    "def vectorize_movie_scripts(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def stringProcessing(s):\n",
    "    s = re.sub(r\"\\'\", \"\", s)\n",
    "    s = re.sub(r'\\n', ' ', s)\n",
    "    s = re.sub(r'\\t', '', s)\n",
    "    s = re.sub(r\"\\[[^[]*\\]\", '', s)\n",
    "    s = re.sub(r'[^\\w\\s]', ' ', s)\n",
    "    s = re.sub(r' +', ' ', s)\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "bourne.txt [ 0.8105246  -0.14522228  0.04622167  0.6015572  -0.06470785  0.4042588\n",
      " -0.9558731  -5.616686  ]\n",
      "deadpoets.txt [ 0.35768542  0.39150706 -0.2343423  -0.12185942  0.01421665  0.24212293\n",
      " -0.83104455 -3.7448676 ]\n",
      "fellowship.txt [ 0.35895628  0.38992435 -0.23346119 -0.12002181  0.01399616  0.24292083\n",
      " -0.8309826  -3.7525668 ]\n",
      "forrest.txt [ 0.7703525  -0.09797597  0.02146828  0.5375512  -0.05771305  0.38985664\n",
      " -0.9446568  -5.449433  ]\n",
      "goodwillhunting.txt [ 0.35877627  0.39228076 -0.23444021 -0.12103412  0.01406391  0.24256587\n",
      " -0.8322054  -3.7557096 ]\n",
      "incredibles.txt [ 0.90325046 -0.25450322  0.10338986  0.7495802  -0.08085938  0.43721765\n",
      " -0.9819761  -5.9998817 ]\n",
      "jedi.txt [ 0.35890222  0.39265424 -0.2345291  -0.12116523  0.01405248  0.24277678\n",
      " -0.83228934 -3.7590182 ]\n",
      "khan.txt [ 0.6793339   0.01042211 -0.03513038  0.391962   -0.04184068  0.35721406\n",
      " -0.9198564  -5.074271  ]\n",
      "shawshank.txt [ 4.1861412e-01  3.2025957e-01 -1.9694099e-01 -2.5005497e-02\n",
      "  3.6155516e-03  2.6403958e-01 -8.4815812e-01 -4.0002837e+00]\n",
      "titanic.txt [ 0.5348119   0.1807688  -0.12432344  0.16153237 -0.01666964  0.30540675\n",
      " -0.8796652  -4.4736834 ]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model = create_model() \n",
    "\n",
    "# df = pd.read_csv(\"https://raw.githubusercontent.com/benbrill/MoodSpace/main/data/trainingSongs_clean.csv\")\n",
    "\n",
    "# data = tf.data.Dataset.from_tensor_slices((df[\"lyrics\"]))\n",
    "# data_vec = data.map(vectorize_headline)\n",
    "# df = pd.DataFrame(model.predict(data_vec))\n",
    "# df.shape\n",
    "for script_path in os.listdir(\"../scripts\"):\n",
    "    with open(f\"../scripts/{script_path}\") as f:\n",
    "        contents = f.read()\n",
    "\n",
    "        contents = stringProcessing(contents)\n",
    "\n",
    "        df = pd.DataFrame({\"lyrics\": [contents]})\n",
    "        \n",
    "\n",
    "\n",
    "        data = tf.data.Dataset.from_tensor_slices((df[\"lyrics\"]))\n",
    "        vectorize_layer.adapt(data)\n",
    "        data_vec = data.map(vectorize_headline)\n",
    "\n",
    "        print(script_path, model.predict(data_vec)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(264, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}